{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text Summarization Project\n",
    "\n",
    "Gruppo di Lavoro:\n",
    "*   Pierfrancesco Lindia\n",
    "*   Cristian Tedesco\n",
    "*   Nabil Larhram\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "8GMGWp4jxDO5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wRm6OQWLqVZK",
    "outputId": "7e4fa036-53f6-4559-e70e-83ee9100da45"
   },
   "outputs": [],
   "source": [
    "# BLOCCO 0 — Install + GPU check\n",
    "!pip -q install transformers datasets evaluate accelerate sentencepiece\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 0 — Setup dell’ambiente e verifica GPU\n",
    "\n",
    "In questo notebook utilizziamo Google Colab con accelerazione GPU per eseguire il fine-tuning di un modello di summarization e la successiva valutazione comparativa.\n",
    "\n",
    "**Obiettivi di questo blocco**\n",
    "- Installare le librerie necessarie per:\n",
    "  - gestione dataset ( `datasets`)\n",
    "  - training/inferenza del modello ( `transformers`, `accelerate`)\n",
    "  - metriche di valutazione ( `evaluate`)\n",
    "  - supporto al tokenizzatore (`sentencepiece`)\n",
    "- Verificare che l’ambiente disponga di **CUDA** e identificare la GPU disponibile, poiché il fine-tuning su CPU sarebbe molto più lento.\n",
    "\n",
    "**Output atteso**\n",
    "Il comando stampa:\n",
    "- se CUDA è disponibile (`True/False`)\n",
    "- il modello di GPU assegnato (nel nostro caso **NVIDIA A100-SXM4-40GB**)\n",
    "- la versione di PyTorch in uso\n",
    "\n",
    "Questa verifica garantisce che le fasi successive (fine-tuning e generazione) possano essere eseguite in modo efficiente e riproducibile.\n"
   ],
   "metadata": {
    "id": "VLrql6C1x6sl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 1 — Config + seed\n",
    "\n",
    "from transformers import set_seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# ===== Valutazione finale (come prima) =====\n",
    "N_EVAL = 500  # numero esempi validation per confronto ROUGE\n",
    "\n",
    "# ===== Fine-tuning (nostro) =====\n",
    "N_TRAIN = 20000      # subset train (20k è ottimo per progetto)\n",
    "N_VAL_FT = 1000      # validation per monitorare training\n",
    "\n",
    "MAX_SOURCE_LEN = 512   # input articolo (training)\n",
    "MAX_TARGET_LEN = 128   # lunghezza riassunto (training)\n",
    "\n",
    "# Modello base da fine-tunare (scelta sicura)\n",
    "BASE_MODEL = \"facebook/bart-base\"\n",
    "\n",
    "# Cartella dove salvare il modello fine-tunato\n",
    "FT_DIR = \"./bart_finetuned_cnn\"\n",
    "\n",
    "print(\"Seed:\", SEED)\n",
    "print(\"BASE_MODEL:\", BASE_MODEL)\n",
    "print(\"N_TRAIN:\", N_TRAIN, \"| N_VAL_FT:\", N_VAL_FT, \"| N_EVAL:\", N_EVAL)\n",
    "print(\"MAX_SOURCE_LEN:\", MAX_SOURCE_LEN, \"| MAX_TARGET_LEN:\", MAX_TARGET_LEN)\n",
    "print(\"FT_DIR:\", FT_DIR)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ckne4nEBrAGr",
    "outputId": "f8ff4ade-d816-4abe-b430-27c9c5f1814a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 1 — Configurazione dell’esperimento e riproducibilità\n",
    "\n",
    "In questa sezione definiamo i parametri globali dell’esperimento, in modo da rendere il workflow **riproducibile** e controllare i principali aspetti computazionali (dimensione dei subset, lunghezze massime, ecc.).\n",
    "\n",
    "### Seed e riproducibilità\n",
    "Impostiamo un seed fisso (`SEED = 42`) con `set_seed(SEED)` per rendere deterministiche (a parità di ambiente) operazioni come lo **shuffle** del dataset e alcune componenti stocastiche della generazione.\n",
    "\n",
    "### Dimensioni dei campioni\n",
    "Per mantenere il progetto computazionalmente sostenibile, utilizziamo subset controllati:\n",
    "- `N_TRAIN = 20000`: numero di esempi usati per il fine-tuning (subset di *train*)\n",
    "- `N_VAL_FT = 1000`: subset di *validation* usato durante il training per monitorare le performance\n",
    "- `N_EVAL = 500`: subset di *validation* usato per la valutazione comparativa finale (ROUGE)\n",
    "\n",
    "Questa separazione consente di:\n",
    "1) addestrare il modello su un campione ampio ma gestibile,\n",
    "2) monitorare il training su un validation set dedicato,\n",
    "3) confrontare i sistemi su un evaluation set fisso e replicabile.\n",
    "\n",
    "### Vincoli di lunghezza (token)\n",
    "Impostiamo vincoli coerenti con il training di modelli seq2seq:\n",
    "- `MAX_SOURCE_LEN = 512`: massimo numero di token per l’input (articolo)\n",
    "- `MAX_TARGET_LEN = 128`: massimo numero di token per l’output (riassunto)\n",
    "\n",
    "### Modello di partenza e checkpoint\n",
    "- `BASE_MODEL`: checkpoint di partenza su cui eseguiamo fine-tuning.\n",
    "- `FT_DIR`: directory in cui salviamo il modello fine-tunato, da riutilizzare nelle fasi di inferenza e valutazione.\n",
    "\n",
    "L’output stampato a fine blocco riepiloga i parametri scelti, così da documentare chiaramente la configurazione dell’esperimento.\n"
   ],
   "metadata": {
    "id": "c2DVLvu5yb1K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 2 — Load dataset + subset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "train_raw = dataset[\"train\"].shuffle(seed=SEED).select(range(N_TRAIN))\n",
    "val_raw_ft = dataset[\"validation\"].shuffle(seed=SEED).select(range(N_VAL_FT))\n",
    "\n",
    "val_eval = dataset[\"validation\"].shuffle(seed=SEED).select(range(N_EVAL))\n",
    "articles = list(val_eval[\"article\"])\n",
    "references = list(val_eval[\"highlights\"])\n",
    "\n",
    "print(\"Train subset:\", len(train_raw))\n",
    "print(\"Val (for FT) subset:\", len(val_raw_ft))\n",
    "print(\"Val (for eval) subset:\", len(val_eval))\n",
    "\n",
    "print(\"\\nKeys:\", train_raw[0].keys())\n",
    "print(\"\\nArticle preview:\\n\", train_raw[0][\"article\"][:400])\n",
    "print(\"\\nHighlights preview:\\n\", train_raw[0][\"highlights\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690,
     "referenced_widgets": [
      "5318510e77ef4db699dfeb864eabcb2b",
      "a38eff6007444a139b6df74295213d40",
      "a45266dbacc1482090d98da8a4f0bf65",
      "660004e0e55d44c69705b8a0ce6dc809",
      "59a89ab7f4904698a0af300255a50a87",
      "37e99e744b9c483fabad264bd51a8442",
      "7876ab618d444261b47007c2d83105e9",
      "da9f49b9cfd74231898051ab4d0a23cf",
      "63590fce774c4403a39ca24d5202d6cc",
      "2fd1501bd08344cc8f5d244bbae238bc",
      "cf615745d9994e0e9987d9b339e18069",
      "9ab26685392e4900b9d4fbac0891b93e",
      "c90b67adae8a4c0fb3e6636f4065199d",
      "280366717da044fabe58a8290f72173a",
      "399fb15b8e1f43eea82b0cf2b904e01c",
      "eaef841b04354f309facc96fa27854be",
      "76cbf78610b849a396969835e771899e",
      "bc5f7f2d453e4d879ca5cd484d477424",
      "e58ca1410d88483388b7ebbc5d1394f8",
      "8e3001338658480c853f109b3fada515",
      "893b7265d7a9483f832553232f65cfce",
      "c3494cba89544beb813e01ebdb8441d2",
      "261f7787b03849f793f3bbc2e8c7b89b",
      "6cc029e09ba54719930a0aead7114b5a",
      "f37c0fc6599044f78b7848a7cfd06d11",
      "c009e13e841a428480a6bb12f0bf95bf",
      "55a142dc9bb6431f84293f15444ce636",
      "a1515cc5368a4f358d5c4394288bdff3",
      "cefba4eb3aaa47f38660046bdf8a6f3f",
      "bf05f18b96224135b5fc61bd116c55ac",
      "1806ae6586c74f8986e7d375c55e37a0",
      "635fdda9e53f4b8a88f0fd6e54cb2cb7",
      "2a62ffbca2f44783aac8a391e255a502",
      "f18a89afb6ea43c9900d51813fcf80e4",
      "0d41ace1f222460d9308595f80669460",
      "7a2f097de75f43c29d70d89121d16b80",
      "087e546a5a8b4d708758e3507783cf32",
      "f066c4cf1e414487a37e888e4db8ab87",
      "e5d8c9aabca141b6be8a774792abac18",
      "ff4af52425324c128aa50fd3eae93079",
      "52a5cc563d61433e8b2e555100a31478",
      "eb53e3ff4efd4ea4bcc927d27f9f41c1",
      "06207d197b7844d0b617b0ab38d90b3a",
      "4f9c5bd46e4149e29651ea17c66aaf03",
      "7ef421ad370b46bb9a4fa81631e8e6ac",
      "1c30e65099de42d79d930e3aa1dd6c19",
      "4f5cdd6ee674492bb7882e0215d71e50",
      "b13dc94230a44d1b8c10fb17ec7e22a3",
      "cffd5a12b3624f7a8049579ad0f0f67c",
      "67bd82aa570c484489258f088856cbfb",
      "d6aef338aae94751b3fc708628501443",
      "032de783f7384b08bd42c17dafb31f67",
      "880e5df7116641e5a2932f9cdea9a06e",
      "39e9f55bdb0949d28fe9dcfe534d9cf2",
      "4c8ee976412d4e618ffd5cf1e21db5ee",
      "aa4f8d4c0e224b76a995b96ae4ea9cfe",
      "600d66b0cec24bb0bbc3dd9560a5bf50",
      "d69a766d1c2c4c91a40ae1a8115eaeb6",
      "6f3d2a28567841c7b78308662a86817e",
      "c8144c674b1a4c14bfdf2416aafc5bf1",
      "334cd53e879c450aad58630eaaf04b3d",
      "1131de52e37b4fa8a9cf2a829207df72",
      "1bda8f8f5c1245e1ab60f2c8622a6c80",
      "77abe28f7efb4b47b53be37584acdc4e",
      "a950a0adff36429880526102d8f48104",
      "dd5c6955e18f432398be43542ec5d426",
      "543721389a4a4f1daf27b0c9c7215b8d",
      "10d0762365a44afab94464d1255c33cb",
      "30a4fad932264d35869a0bc57a46ba03",
      "cf8ee6556cde46bbac61c454ca1b27d8",
      "c820a3d2ecff417e87cb7a4a4e7b787e",
      "78366df25f214dc985c7fb42e861f1c1",
      "036b82403cc54816bcae77208e17f698",
      "575225f605304aaa8bb915bd54cadb71",
      "00ac9031fe8546a48b7d1654ce9fc599",
      "1e4072ab652e4faba764d6eb76391054",
      "4c141d37fc9d4380a250d5cc8cf29609",
      "a5bb6cbe5b2a491cbcd5374e01a822b1",
      "d9d8c56710a94fedbe7040178dccd0ef",
      "fb79cbc8e6164546a049d4a748e06f12",
      "3382653032434f77a5ed388c4a637963",
      "fa3c2b598e1849198de01e1f33bdb5b6",
      "56c6eacf92664fccb91f0c3967c0a2ad",
      "f7ce2a9371f147c781d52063402f38f8",
      "aea03d980a43465cb38711c2d1dcf37a",
      "890545a524114d16aadd87696107dddd",
      "2b9634e1dbec4dd8bbf2f975aa2edfac",
      "deee4a63afb34ab69e8c84881eca806b",
      "8a22ba64716b45c39414d8d87a6cab27",
      "b9cac03789304e4b8d621104599b9b15",
      "d39ca9c7dddd41bf97893f89547e46e6",
      "ed8644f7762b4a998c7341d9061f67ab",
      "8fbc75b8a64f485e9b5b294997d812d3",
      "d932ec98f72d453696a9b5957d7210f6",
      "659676acb52240a4adb9c7bdff430f24",
      "deecebf549b9476faaaed2bfc88f7258",
      "176ef0394b1a4b04aec646f714b875a9",
      "a2901eba7e6645079895e15f6c6d37e6",
      "3550f089b3474037849cdfda53e31d0d"
     ]
    },
    "id": "ZMKz8BPhrWKC",
    "outputId": "a5df60f5-72a4-4476-e91d-032df93c4136"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 2 — Caricamento del dataset e costruzione dei subset\n",
    "\n",
    "In questo blocco carichiamo il dataset **CNN/DailyMail (v3.0.0)** tramite la libreria `datasets` e costruiamo i subset necessari per le diverse fasi del workflow (fine-tuning e valutazione finale).\n",
    "\n",
    "### Caricamento del dataset\n",
    "`load_dataset(\"cnn_dailymail\", \"3.0.0\")` scarica il dataset (la prima volta) e lo memorizza in cache. Il dataset è organizzato nei tre split standard:\n",
    "- `train`\n",
    "- `validation`\n",
    "- `test`\n",
    "\n",
    "> Nota: il warning relativo a `HF_TOKEN` indica solo che non stiamo autenticando Colab su Hugging Face. Per dataset pubblici l’autenticazione **non è necessaria** e l’avviso non influisce sull’esecuzione.\n",
    "\n",
    "### Costruzione dei subset (riproducibili)\n",
    "Per rendere l’esperimento replicabile e sostenibile:\n",
    "- applichiamo `shuffle(seed=SEED)` per mescolare gli esempi in modo deterministico;\n",
    "- selezioniamo poi un numero fisso di istanze con `.select(range(...))`.\n",
    "\n",
    "Otteniamo così:\n",
    "- `train_raw`: subset di train di dimensione `N_TRAIN` per il fine-tuning;\n",
    "- `val_raw_ft`: subset di validation di dimensione `N_VAL_FT` per monitorare il training;\n",
    "- `val_eval`: subset di validation di dimensione `N_EVAL` per la valutazione comparativa finale (ROUGE).\n",
    "\n",
    "### Estrazione delle colonne di interesse\n",
    "Dal subset di evaluation estraiamo:\n",
    "- `articles`: i testi completi degli articoli (input del sistema)\n",
    "- `references`: i riassunti di riferimento (*highlights*), usati come gold standard per il confronto con ROUGE\n",
    "\n",
    "Infine stampiamo:\n",
    "- le dimensioni dei subset creati,\n",
    "- le chiavi disponibili in un record (`article`, `highlights`, `id`),\n",
    "- un’anteprima di un articolo e del relativo riassunto di riferimento.\n"
   ],
   "metadata": {
    "id": "_wA8QDO7y3wJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 3 — Tokenizer + preprocessing\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"article\"],\n",
    "        max_length=MAX_SOURCE_LEN,\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=batch[\"highlights\"],\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_raw.map(preprocess, batched=True, remove_columns=train_raw.column_names)\n",
    "val_tok_ft = val_raw_ft.map(preprocess, batched=True, remove_columns=val_raw_ft.column_names)\n",
    "\n",
    "print(\"Tokenized train example keys:\", train_tok[0].keys())\n",
    "print(\"input_ids length:\", len(train_tok[0][\"input_ids\"]))\n",
    "print(\"labels length:\", len(train_tok[0][\"labels\"]))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261,
     "referenced_widgets": [
      "bd25df4128df4047a216bf106d4fef26",
      "2cdc1c7000584fceb963c1732755e9d3",
      "86c3418786c84b1faf3f6ed8ae2b1e16",
      "cb8197f303cc4dffad8cfc5fd40dd182",
      "1c58e29d002948679dcf0137a391c382",
      "2de3b5fc071b48be8787729e0a2d2aef",
      "c71ffa94a9af462c9674fef97b99b9b7",
      "6c164293aba64bfca7b4ee59d83a3305",
      "806d13c4b4664ff686474880a97797aa",
      "33ff581b248e4b9d96a4cc63e826403a",
      "eb6bb35c524741bc91a0e7ec2b2d5ae8",
      "df9ddd0cf7a9406581491bea4aa2983e",
      "48c11bdfec24499d9647b0d7ebc7154e",
      "20a60d7cb3fa481e9021d2b45012b5de",
      "6f7f211ed51943cf9d915209b489a4cc",
      "6bb7cbab1b5b4b67b7a3870b236a74fb",
      "a36ceddc855c4a74ac6469f45cc6d68a",
      "c6bbcf5f371c4560b6af14096d038e8a",
      "03352d4310f84ade8632b0cba438eb8a",
      "f5228981c3624d85967c8caf81a545b8",
      "bc567326a11846ce860c05bfe031e57f",
      "a9235aec7b6b4ca6a2f00722677992eb",
      "5f672cc7d59f4084a0868ced9734579f",
      "2a8cf441c03e4a0ab418b661704a9731",
      "22cde40a438d4a848c17bd19772588be",
      "d4744671544a4ebc896cc6e283f5de94",
      "c3ef9b393db84e7bbd2cbba74e538572",
      "b5bce383f49a40448373052f9e105d31",
      "ebebcc6253b246ce8c0cf852a7c33410",
      "c6c15cac1d484660b8a3a5139eacc01e",
      "39879f52cd5f49078c24478b974deae4",
      "d5efd456982c4db3aebc1671a5cf5737",
      "e526b590f1af412694e6550416126d9b",
      "a1689373de564ae29dc26bc775783860",
      "17aeaca6670f48b283a8c0657cb88a12",
      "89dac9439f5d44cfbdf4d8404e467bbe",
      "161c95147abe46af973420e95a230ba0",
      "adc7c2a65db34941b1def9bb7741fff0",
      "e599ddd51941458e95ff36cdefa59fe3",
      "bfbe4290a4e14cc4a65693022d3c1f76",
      "cf64aeec5def411ca2e42b9c56e36fa9",
      "1d71c3c93597455491344619996457d0",
      "d2aa59d6cfa54cf08c596cf53de7d107",
      "21b6bcbf77f44424a644dc4144e1f2e2",
      "606e792bb4e542718459e8c3dc4c2ef5",
      "e85b0c4307a8407bb7f968386aabbe30",
      "d68525522caf47f4af041a0f24dd76ba",
      "e4835480171645f7935eebdce13bfc9d",
      "241a215a2e3049b09dba48bdff2ca1bc",
      "f5d32085aee64fc0afb39eb8497376ad",
      "d97926a1ee0f442a9016c9505b509983",
      "aeb11d1c329645509dd099b16e5112c6",
      "02be2b7086964b6f8fba36cad7c413a2",
      "5bbb6ac04c5f4b728de8134ca213ca29",
      "8797fd5507b64253a0ba5c3c5426aaa4",
      "a4281bc3928243f5a7c63cb30547be51",
      "10bde10866db4643bd04367b1bc7ac26",
      "35ff0a8bb15c41048022b1ad2311a793",
      "d3f660f849cb409ca26cfeb63cdbae87",
      "7ca92a3458ca417283e59eb498b3ef22",
      "38acec9a174842728945f3e1a329f677",
      "4f6f7317caec4acb9faeb348a41be22e",
      "16f1b0e08c194da39a12628783bf2d5b",
      "8993cad7d0ee4bee849fb5d13f87440f",
      "035386add9124f23bc3f4a4ab7b9d724",
      "d39cab0c90e14063825315fbef1b1a3b"
     ]
    },
    "id": "XnkI23OrreLb",
    "outputId": "e2da0c71-3b2b-4bd5-9f63-086adb1b9ed7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 3 — Tokenizzazione e preprocessing per il fine-tuning\n",
    "\n",
    "In questo blocco prepariamo i dati nel formato richiesto da un modello **seq2seq** (encoder–decoder) per la summarization.\n",
    "\n",
    "### Tokenizer\n",
    "Carichiamo il tokenizer associato al checkpoint `BASE_MODEL`. Il tokenizer converte testo → sequenze di token/ID e gestisce il vocabolario e le regole di segmentazione coerenti con il modello.\n",
    "\n",
    "### Preprocessing (input e target)\n",
    "Definiamo una funzione `preprocess` che, per ogni batch:\n",
    "- tokenizza gli **articoli** (`article`) come input del modello:\n",
    "  - `max_length=MAX_SOURCE_LEN`\n",
    "  - `truncation=True` per troncare gli articoli troppo lunghi al limite scelto\n",
    "- tokenizza i **riassunti di riferimento** (`highlights`) come target:\n",
    "  - `text_target=...` indica che stiamo tokenizzando la sequenza “label” (output atteso)\n",
    "  - `max_length=MAX_TARGET_LEN`\n",
    "  - `truncation=True`\n",
    "\n",
    "Il risultato è un dizionario contenente:\n",
    "- `input_ids` e `attention_mask` (input per l’encoder)\n",
    "- `labels` (token del riassunto, usati per calcolare la loss in training)\n",
    "\n",
    "### Applicazione al dataset\n",
    "Applichiamo `map(..., batched=True)` ai subset `train_raw` e `val_raw_ft`, rimuovendo le colonne originali (`remove_columns=...`) per ottenere dataset già pronti per il Trainer.\n",
    "\n",
    "### Check finale\n",
    "Stampiamo:\n",
    "- le chiavi disponibili nell’esempio tokenizzato,\n",
    "- la lunghezza di `input_ids` e `labels`,\n",
    "per verificare che i vincoli di lunghezza siano rispettati e che la struttura sia corretta.\n"
   ],
   "metadata": {
    "id": "srCieyz8y-4S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 4 — Fine-tuning (compatibile)\n",
    "!pip -q install rouge_score\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return rouge.compute(predictions=pred_str, references=label_str, use_stemmer=True)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=FT_DIR,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok_ft,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(FT_DIR)\n",
    "tokenizer.save_pretrained(FT_DIR)\n",
    "\n",
    "print(\"Saved model to:\", FT_DIR)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "jeT5TEIgrwuO",
    "outputId": "78641535-c661-40a9-f796-8427a62e501b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 4 — Fine-tuning del modello (addestramento supervisionato)\n",
    "\n",
    "In questo blocco eseguiamo il **fine-tuning supervisionato** del modello seq2seq sul task di summarization, utilizzando coppie *(articolo → highlights)*. L’obiettivo è adattare il modello al dataset scelto aggiornando i pesi tramite ottimizzazione della loss (cross-entropy sui token target).\n",
    "\n",
    "### Caricamento del modello e data collator\n",
    "- `AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)` carica l’architettura encoder–decoder.\n",
    "- `DataCollatorForSeq2Seq` gestisce il padding dinamico all’interno di ogni batch e prepara correttamente le `labels` (inclusa la mascheratura con `-100` per i token di padding, così da non contribuire alla loss).\n",
    "\n",
    "### Monitoraggio tramite ROUGE durante il training\n",
    "Definiamo `compute_metrics` per calcolare ROUGE durante la valutazione intermedia:\n",
    "- `predict_with_generate=True` abilita la generazione dei riassunti in fase di eval.\n",
    "- Le `labels` contengono `-100` nei punti di padding; per poter decodificare correttamente le sequenze di riferimento sostituiamo `-100` con `pad_token_id`.\n",
    "- Decodifichiamo predizioni e riferimenti con `batch_decode` e calcoliamo ROUGE con stemming (`use_stemmer=True`) per ridurre la sensibilità a variazioni morfologiche.\n",
    "\n",
    "### Scelte degli iperparametri (TrainingArguments)\n",
    "- `learning_rate=2e-5`: valore tipico per fine-tuning stabile di modelli transformer.\n",
    "- `per_device_train_batch_size=4` e `gradient_accumulation_steps=4`: l’accumulo del gradiente simula un batch effettivo più grande (batch effettivo ≈ 16), mantenendo sotto controllo la memoria.\n",
    "- `num_train_epochs=1`: impostazione iniziale conservativa, adeguata a un primo fine-tuning su subset.\n",
    "- Valutazione e salvataggio “a step”:\n",
    "  - `eval_strategy=\"steps\"`, `eval_steps=500` per monitorare periodicamente\n",
    "  - `save_strategy=\"steps\"`, `save_steps=500` per salvare checkpoint intermedi\n",
    "- `fp16=True` (se CUDA disponibile): abilita mixed precision per accelerare training e ridurre VRAM.\n",
    "\n",
    "### Training e salvataggio del checkpoint\n",
    "Con `Seq2SeqTrainer` avviamo l’addestramento (`trainer.train()`). Al termine salviamo:\n",
    "- i pesi del modello fine-tunato (`trainer.save_model(FT_DIR)`)\n",
    "- il tokenizer associato (`tokenizer.save_pretrained(FT_DIR)`)\n",
    "\n",
    "Il checkpoint salvato verrà poi ricaricato nei blocchi successivi per la g\n"
   ],
   "metadata": {
    "id": "yMHHh6qVzJYP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 5 — Load fine-tuned model + pipeline\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "model_name = FT_DIR\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(model_name)\n",
    "model_ft = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model_ft,\n",
    "    tokenizer=tokenizer_ft,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Loaded model from:\", model_name)\n",
    "print(\"Device:\", \"GPU\" if device == 0 else \"CPU\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5Chmu5IuLCp",
    "outputId": "a9c3d65b-d28f-402c-a205-8e2fa4dd6d23"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 5 — Caricamento del checkpoint fine-tunato e creazione della pipeline\n",
    "\n",
    "Dopo il fine-tuning, ricarichiamo il modello salvato su disco e lo configuriamo per l’**inferenza** (generazione dei riassunti) tramite la `pipeline` di Hugging Face.\n",
    "\n",
    "### Selezione del device\n",
    "Impostiamo `device` in base alla disponibilità della GPU:\n",
    "- `device = 0` se CUDA è disponibile (uso della GPU)\n",
    "- `device = -1` altrimenti (CPU)\n",
    "\n",
    "### Caricamento del modello fine-tunato\n",
    "Carichiamo dal percorso `FT_DIR`:\n",
    "- `tokenizer_ft`: il tokenizer salvato insieme al modello\n",
    "- `model_ft`: i pesi del modello dopo il fine-tuning\n",
    "\n",
    "Questo garantisce coerenza tra tokenizzazione e parametri del modello durante la generazione.\n",
    "\n",
    "### Pipeline di summarization\n",
    "Creiamo una pipeline `summarization` che incapsula:\n",
    "- preprocessing (tokenizzazione)\n",
    "- generazione del riassunto\n",
    "- postprocessing (decodifica)\n",
    "\n",
    "In questo modo, nei blocchi successivi possiamo richiamare il riassunto con una singola chiamata, mantenendo il codice più leggibile e modulare.\n",
    "\n",
    "A fine blocco stampiamo:\n",
    "- il percorso del checkpoint caricato\n",
    "- se l’inferenza avverrà su GPU o CPU\n"
   ],
   "metadata": {
    "id": "tWpvowKJzUy3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 6 — Baselines + chunking + summarization\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "MAX_INPUT_TOKENS = 1024\n",
    "CHUNK_TOKENS = 900\n",
    "\n",
    "_SENT_SPLIT = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return _SENT_SPLIT.split(text)\n",
    "\n",
    "def baseline_lead3(text: str, n_sent: int = 3) -> str:\n",
    "    sents = split_sentences(text)\n",
    "    if len(sents) <= n_sent:\n",
    "        return (text or \"\").strip()\n",
    "    return \" \".join(sents[:n_sent]).strip()\n",
    "\n",
    "def baseline_tfidf_extractive(text: str, n_sent: int = 3) -> str:\n",
    "    sents = [s.strip() for s in split_sentences(text) if s.strip()]\n",
    "    if len(sents) <= n_sent:\n",
    "        return \" \".join(sents).strip()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(sents)\n",
    "\n",
    "    doc_vec = np.asarray(X.sum(axis=0))\n",
    "    if doc_vec.ndim == 1:\n",
    "        doc_vec = doc_vec.reshape(1, -1)\n",
    "\n",
    "    sims = cosine_similarity(X, doc_vec).ravel()\n",
    "    top_idx = np.argsort(-sims)[:n_sent]\n",
    "    top_idx_sorted = sorted(top_idx.tolist())\n",
    "\n",
    "    return \" \".join(sents[i] for i in top_idx_sorted).strip()\n",
    "\n",
    "def chunk_by_tokens(text: str, tok, chunk_tokens: int = 900):\n",
    "    ids = tok.encode(text, add_special_tokens=False)\n",
    "    return [\n",
    "        tok.decode(ids[i:i + chunk_tokens], skip_special_tokens=True)\n",
    "        for i in range(0, len(ids), chunk_tokens)\n",
    "    ]\n",
    "\n",
    "def _dynamic_max_len(text: str, tok, base_max: int, min_len: int, ratio: float = 0.6) -> int:\n",
    "    in_len = len(tok.encode(text, add_special_tokens=False))\n",
    "    cap = max(5, in_len - 1)\n",
    "    proposed = int(in_len * ratio)\n",
    "    lower = min(cap, min_len + 5)\n",
    "    return min(base_max, cap, max(lower, proposed))\n",
    "\n",
    "def bart_summary(text: str, max_len: int = 130, min_len: int = 30, hierarchical: bool = True) -> str:\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        n_tokens = len(tokenizer_ft.encode(text, add_special_tokens=False))\n",
    "\n",
    "        if hierarchical and n_tokens > MAX_INPUT_TOKENS:\n",
    "            chunks = chunk_by_tokens(text, tokenizer_ft, CHUNK_TOKENS)\n",
    "            partials = []\n",
    "\n",
    "            for ch in chunks:\n",
    "                dyn_max = _dynamic_max_len(ch, tokenizer_ft, base_max=max_len, min_len=min_len, ratio=0.6)\n",
    "                out = summarizer(ch, max_length=dyn_max, min_length=min_len, do_sample=False)\n",
    "                partials.append(out[0][\"summary_text\"])\n",
    "\n",
    "            merged = \" \".join(partials)\n",
    "\n",
    "            final_min = 40\n",
    "            final_base_max = 120\n",
    "            dyn_final_max = _dynamic_max_len(merged, tokenizer_ft, base_max=final_base_max, min_len=final_min, ratio=0.7)\n",
    "\n",
    "            out_final = summarizer(merged, max_length=dyn_final_max, min_length=final_min, do_sample=False)\n",
    "            return out_final[0][\"summary_text\"].strip()\n",
    "\n",
    "        dyn_max = _dynamic_max_len(text, tokenizer_ft, base_max=max_len, min_len=min_len, ratio=0.6)\n",
    "        out = summarizer(text, max_length=dyn_max, min_length=min_len, do_sample=False)\n",
    "        return out[0][\"summary_text\"].strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[BART] Error: {e}\")\n",
    "        return \"Error in summary generation.\"\n"
   ],
   "metadata": {
    "id": "9kookTtguZIA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 6 — Baseline estrattive, gestione di input lunghi e funzione di generazione\n",
    "\n",
    "In questo blocco definiamo:\n",
    "1) due **baseline estrattive** (non neurali) per il confronto,\n",
    "2) una strategia robusta per gestire articoli lunghi,\n",
    "3) una funzione unificata per generare riassunti con il modello fine-tunato.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Baseline estrattiva Lead-3\n",
    "`baseline_lead3` implementa una strategia semplice e molto usata nel dominio news:\n",
    "- segmenta l’articolo in frasi,\n",
    "- restituisce la concatenazione delle **prime 3 frasi**.\n",
    "\n",
    "È una baseline forte perché molti articoli giornalistici seguono una struttura a “piramide invertita”, in cui le informazioni principali compaiono all’inizio.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Baseline estrattiva TF-IDF + similarità coseno\n",
    "`baseline_tfidf_extractive`:\n",
    "- divide l’articolo in frasi,\n",
    "- costruisce una rappresentazione TF-IDF delle frasi,\n",
    "- calcola un vettore “documento” aggregato (somma delle feature),\n",
    "- seleziona le frasi con maggiore similarità coseno rispetto al vettore documento.\n",
    "\n",
    "Questo approccio è interpretabile e completamente estrattivo, ma può produrre output meno coesi (frasi non contigue o ridondanti).\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Gestione degli input lunghi: chunking token-based\n",
    "I modelli transformer hanno un limite massimo di lunghezza in input; per articoli molto lunghi utilizziamo una strategia di **chunking basata sui token**:\n",
    "- `chunk_by_tokens` spezza l’articolo in segmenti da `CHUNK_TOKENS` token,\n",
    "- questo evita errori di overflow e garantisce compatibilità con il modello.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Controllo dinamico della lunghezza dell’output\n",
    "La funzione `_dynamic_max_len` imposta un `max_length` proporzionale alla lunghezza dell’input:\n",
    "- evita riassunti troppo lunghi rispetto al testo,\n",
    "- mantiene coerenza tra `min_length` e `max_length`,\n",
    "- riduce generazioni “sproporzionate” su input molto corti o molto lunghi.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Funzione `bart_summary`: modalità standard e gerarchica\n",
    "`bart_summary` genera riassunti con due modalità:\n",
    "- **Standard**: se l’articolo è entro `MAX_INPUT_TOKENS`, genera direttamente un riassunto.\n",
    "- **Gerarchica**: se l’articolo supera il limite:\n",
    "  1) genera un riassunto per ciascun chunk,\n",
    "  2) concatena i riassunti parziali,\n",
    "  3) genera un **riassunto finale** sul testo aggregato.\n",
    "\n",
    "In questo modo otteniamo un comportamento robusto anche su articoli lunghi, mantenendo la pipeline stabile in inferenza e comparabile con le baseline.\n"
   ],
   "metadata": {
    "id": "fPzoUC2Fzk_f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 7 — Generate summaries + ROUGE\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "pred_bart, pred_lead3, pred_tfidf = [], [], []\n",
    "\n",
    "for art in tqdm(articles, desc=\"Generating summaries\"):\n",
    "    pred_bart.append(bart_summary(art, hierarchical=True))\n",
    "    pred_lead3.append(baseline_lead3(art, n_sent=3))\n",
    "    pred_tfidf.append(baseline_tfidf_extractive(art, n_sent=3))\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def rouge_scores(preds, refs):\n",
    "    return rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "\n",
    "scores_bart = rouge_scores(pred_bart, references)\n",
    "scores_lead3 = rouge_scores(pred_lead3, references)\n",
    "scores_tfidf = rouge_scores(pred_tfidf, references)\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"system\": f\"BART fine-tuned ({BASE_MODEL})\", **scores_bart},\n",
    "    {\"system\": \"Baseline Lead-3\", **scores_lead3},\n",
    "    {\"system\": \"Baseline TF-IDF extractive\", **scores_tfidf},\n",
    "])\n",
    "\n",
    "cols = [\"system\", \"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "print(df[cols].to_string(index=False))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b73dc5175cb041ceb21002f8edc3437b",
      "de45213bae0c422faf625317f09a2b43",
      "d34bc31bf3bd4d11a5a78c0f048c3470",
      "4d47449c97e6467f9ed777e3b0b8109f",
      "2e96944d503b47ca99c5417715157c39",
      "2708629570d142a7ad8b04ea4b22e497",
      "fec9cc11cf0e4aaeb1cedb5e30329a3c",
      "4afb7131c57c47f48cd416a77552035c",
      "b4a2c336396a4e5e86e7bb584d7705f8",
      "ba8e6f6d6427459c97c19fc33bed8e86",
      "10163b77dff64e5d89bc795851a76088"
     ]
    },
    "id": "TIPLoJ61ujDK",
    "outputId": "d27d9c9e-0cd2-409a-8255-0469f7b732f6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 7 — Generazione dei riassunti e valutazione quantitativa (ROUGE)\n",
    "\n",
    "In questo blocco eseguiamo la **valutazione comparativa finale** sui `N_EVAL` articoli dello split *validation* (subset fisso e riproducibile definito nel Blocco 2).\n",
    "\n",
    "### 1) Generazione delle predizioni\n",
    "Per ogni articolo in `articles` generiamo tre riassunti:\n",
    "- `pred_bart`: riassunto prodotto dal **modello fine-tunato** tramite `bart_summary` (con gestione gerarchica degli input lunghi)\n",
    "- `pred_lead3`: baseline estrattiva **Lead-3**\n",
    "- `pred_tfidf`: baseline estrattiva **TF-IDF + coseno**\n",
    "\n",
    "Le tre liste di predizioni hanno la stessa cardinalità e sono confrontabili una-a-una con i riferimenti `references` (highlights).\n",
    "\n",
    "### 2) Valutazione con ROUGE\n",
    "Utilizziamo la metrica **ROUGE** per confrontare automaticamente i riassunti generati con i riassunti di riferimento:\n",
    "- **ROUGE-1**: sovrapposizione di unigrammi\n",
    "- **ROUGE-2**: sovrapposizione di bigrammi\n",
    "- **ROUGE-L** e **ROUGE-Lsum**: basate sulla *Longest Common Subsequence*, utili per catturare similarità di struttura e ordine\n",
    "\n",
    "Impostiamo `use_stemmer=True` per ridurre la sensibilità a variazioni morfologiche (es. plurali/tempi verbali).\n",
    "\n",
    "### 3) Tabella riassuntiva dei risultati\n",
    "Raccogliamo i punteggi ROUGE in un `DataFrame` e stampiamo una tabella comparativa con le quattro metriche principali, così da osservare immediatamente le differenze tra:\n",
    "- approccio neurale (modello fine-tunato)\n",
    "- approcci estrattivi (Lead-3 e TF-IDF)\n",
    "\n",
    "Questa sezione costituisce la base quantitativa su cui poggia l’interpretazione dei risultati.\n"
   ],
   "metadata": {
    "id": "gNXn6bzlztky"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BLOCCO 8 — Qualitative check (first 5)\n",
    "\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "_SENT_SPLIT_READABLE = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "def pretty_wrap(text: str, width: int = 90) -> str:\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return \"\\n\".join(textwrap.wrap(text, width=width))\n",
    "\n",
    "W = 90\n",
    "\n",
    "print(\"\\n--- Qualitative examples (first 5) ---\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"Example {i+1}\\n\")\n",
    "    print(\"REFERENCE:\\n\" + pretty_wrap(references[i], width=W) + \"\\n\")\n",
    "    print(\"BART (fine-tuned):\\n\" + pretty_wrap(pred_bart[i], width=W) + \"\\n\")\n",
    "    print(\"LEAD-3:\\n\" + pretty_wrap(pred_lead3[i], width=W) + \"\\n\")\n",
    "    print(\"TF-IDF:\\n\" + pretty_wrap(pred_tfidf[i], width=W))\n",
    "    print(\"-\" * 90)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yy6NU2m-xWiU",
    "outputId": "0562724d-1851-4a7d-bce3-4a9bc8ef0b5c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Blocco 8 — Valutazione qualitativa (ispezione manuale di esempi)\n",
    "\n",
    "Le metriche automatiche (es. ROUGE) misurano principalmente la **sovrapposizione lessicale** tra riassunto generato e riferimento, ma non catturano completamente aspetti come:\n",
    "- scorrevolezza e coerenza discorsiva,\n",
    "- ridondanza,\n",
    "- completezza informativa,\n",
    "- eventuali dettagli aggiunti o omessi.\n",
    "\n",
    "Per questo motivo, affianchiamo alla valutazione quantitativa una breve **ispezione qualitativa**.\n",
    "\n",
    "### Procedura\n",
    "Per i primi 5 esempi del subset di evaluation (`val_eval`):\n",
    "- stampiamo il riassunto di riferimento (`REFERENCE`, ovvero gli *highlights*),\n",
    "- confrontiamo i tre output:\n",
    "  - **BART (fine-tuned)**\n",
    "  - **Lead-3**\n",
    "  - **TF-IDF**\n",
    "\n",
    "### Formattazione dell’output\n",
    "La funzione `pretty_wrap` va a capo automaticamente a una larghezza fissa (`W = 90`), migliorando la leggibilità in console/notebook e facilitando il confronto visivo tra i diversi metodi.\n",
    "\n",
    "### Scopo dell’analisi qualitativa\n",
    "Questa sezione permette di osservare concretamente i diversi trade-off:\n",
    "- Lead-3: forte aderenza all’incipit dell’articolo (approccio posizionale)\n",
    "- TF-IDF: selezione di frasi lessicalmente salienti (approccio basato su similarità)\n",
    "- Modello fine-tunato: maggiore (potenziale) capacità di sintesi e riformulazione\n",
    "\n",
    "Le osservazioni qualitative vengono poi utilizzate per interpretare e contestualizzare i risultati ROUGE ottenuti nel blocco precedente.\n"
   ],
   "metadata": {
    "id": "CcsglcM4z3qO"
   }
  }
 ]
}